# 【任务1 - 线性回归算法梳理】

## 机器学习的一些概念 有监督、无监督、泛化能力、过拟合欠拟合(方差和偏差以及各自解决办法)、交叉验证
1. 监督学习是指从标注数据中学习预测模型的机器学习问题.标注数据表示输入输出的对应关系,预测模型对给定的输入产生相应的输出.监督学习的本质是学习输入到输出的映射的统计规律. 
2. 无监督学习是指从无标注数据中学习预测模型的机器学习问题.无标注数据是自然得到的数据,预测模型表示数据的类别,转换或概率.无监督学习的本质是学习数据中的统计规律或潜在结构.
3. 泛化能力是指由该方法学习到的模型对未知数据的预测能力,是学习方法本质上重要的性质.现实中采用最多的办法是通过测试误差来评价学习方法的泛化能力,即泛化误差,也就是期望风险.
4. 过拟合是指学习时选择的模型所包含的参数过多,以至出现这一模型对已知数据预测得很好,但对未知数据预测得很差的现象.
5. 交叉验证,将数据集分为训练集,验证集和测试集.
(以上解答来自小蓝书)
## 线性回归的原理
1. 试图学得一个通过属性的线性组合来进行预测的函数 y=wx+b
## 线性回归损失函数、代价函数、目标函数
1. 损失函数（Loss Function ）是定义在单个样本上的，算的是一个样本的误差。
2. 代价函数（Cost Function ）是定义在整个训练集上的，是所有样本误差的平均，也就是损失函数的平均。
3. 目标函数（Object Function）定义为：最终需要优化的函数。等于经验风险+结构风险（也就是Cost Function + 正则化项）。
## 优化方法(梯度下降法、牛顿法、拟牛顿法等)
1. 梯度下降法的优化思想是用当前位置负梯度方向作为搜索方向，因为该方向为当前位置的最快下降方向，所以也被称为是最速下降法。  
批量梯度下降---最小化所有训练样本的损失函数，使得最终求解的是全局的最优解，即求解的参数是使得风险函数最小，但是对于大规模样本问题效率低下。  
随机梯度下降---最小化每条样本的损失函数，虽然不是每次迭代得到的损失函数都向着全局最优方向， 但是大的整体的方向是向全局最优解的，最终的结果往往是在全局最优解附近，适用于大规模训练样本情况。
2. 牛顿法是一种在实数域和复数域上近似求解方程的方法。方法使用函数f (x)的泰勒级数的前面几项来寻找方程f (x) = 0的根。牛顿法最大的特点就在于它的收敛速度很快。
3. 拟牛顿法的本质思想是改善牛顿法每次需要求解复杂的Hessian矩阵的逆矩阵的缺陷，它使用正定矩阵来近似Hessian矩阵的逆，从而简化了运算的复杂度。
4. 共轭梯度法是介于最速下降法与牛顿法之间的一个方法，它仅需利用一阶导数信息，但克服了最速下降法收敛慢的缺点，又避免了牛顿法需要存储和计算Hesse矩阵并求逆的缺点，共轭梯度法不仅是解决大型线性方程组最有用的方法之一，也是解大型非线性最优化最有效的算法之一。
5. 启发式方法指人在解决问题时所采取的一种根据经验规则进行发现的方法。其特点是在解决问题时,利用过去的经验,选择已经行之有效的方法，而不是系统地、以确定的步骤去寻求答案。启发式优化方法种类繁多，包括经典的模拟退火方法、遗传算法、蚁群算法以及粒子群算法等等。
　　还有一种特殊的优化算法被称之多目标优化算法，它主要针对同时优化多个目标（两个及两个以上）的优化问题，这方面比较经典的算法有NSGAII算法、MOEA/D算法以及人工免疫算法等。
6. 拉格朗日乘数法
## 线性回归的评估指标
1. MSE, RMSE和MAE
2. R方
## sklearn参数详解
1. KNN
n_neighbors：默认为5，就是k-NN的k的值，选取最近的k个点。  
weights：默认是uniform，参数可以是uniform、distance，也可以是用户自己定义的函数。uniform是均等的权重，就说所有的邻近点的权重都是相等的。distance是不均等的权重，距离近的点比距离远的点的影响大。用户自定义的函数，接收距离的数组，返回一组维数相同的权重。  
algorithm：快速k近邻搜索算法，默认参数为auto，可以理解为算法自己决定合适的搜索算法。除此之外，用户也可以自己指定搜索算法ball_tree、kd_tree、brute方法进行搜索，brute是蛮力搜索，也就是线性扫描，当训练集很大时，计算非常耗时。kd_tree，构造kd树存储数据以便对其进行快速检索的树形数据结构，kd树也就是数据结构中的二叉树。以中值切分构造的树，每个结点是一个超矩形，在维数小于20时效率高。ball tree是为了克服kd树高纬失效而发明的，其构造过程是以质心C和半径r分割样本空间，每个节点是一个超球体。  
leaf_size：默认是30，这个是构造的kd树和ball树的大小。这个值的设置会影响树构建的速度和搜索速度，同样也影响着存储树所需的内存大小。需要根据问题的性质选择最优的大小。  
metric：用于距离度量，默认度量是minkowski，也就是p=2的欧氏距离(欧几里德度量)。  
p：距离度量公式。在上小结，我们使用欧氏距离公式进行距离度量。除此之外，还有其他的度量方法，例如曼哈顿距离。这个参数默认为2，也就是默认使用欧式距离公式进行距离度量。也可以设置为1，使用曼哈顿距离公式进行距离度量。  
metric_params：距离公式的其他关键参数，这个可以不管，使用默认的None即可。  
n_jobs：并行处理设置。默认为1，临近点搜索并行工作数。如果为-1，那么CPU的所有cores都用于并行工作。  
2. KMeans
n_clusters:簇的个数，即你想聚成几类  
init: 初始簇中心的获取方法  
n_init: 获取初始簇中心的更迭次数，为了弥补初始质心的影响，算法默认会初始10次质心，实现算法，然后返回最好的结果。  
max_iter: 最大迭代次数（因为kmeans算法的实现需要迭代）  
tol: 容忍度，即kmeans运行准则收敛的条件  
precompute_distances：是否需要提前计算距离，这个参数会在空间和时间之间做权衡，如果是True 会把整个距离矩阵都放到内存中，auto 会默认在数据样本大于featurs*samples 的数量大于12e6 的时候False,False 时核心实现的方法是利用Cpython 来实现的  
verbose: 冗长模式（不太懂是啥意思，反正一般不去改默认值）  
random_state: 随机生成簇中心的状态条件。  
copy_x: 对是否修改数据的一个标记，如果True，即复制了就不会修改数据。bool 在scikit-learn 很多接口中都会有这个参数的，就是是否对输入数据继续copy 操作，以便不修改用户的输入数据。这个要理解Python 的内存机制才会比较清楚。  
n_jobs: 并行设置  
algorithm: kmeans的实现算法，有：’auto’, ‘full’, ‘elkan’, 其中 ‘full’表示用EM方式实现  
